# Audio Synthesis Experimentation with Generative Adversarial Networks

This repo represents some work I did for a class project for my Machine and Computer Vision course. I went somewhat off the beaten trail and decided rather than attempting a classification problem I would focus on a generative project. I was drawn to the idea of a 
model that could "read" and then 'play" music. Not sheet music, but spectrograms, which are a visual representation of audio, and where pixels do not have a spatial context but instead a time-frequency relationship with a different semantic meaning. I made my own GAN model, so including the generator, discriminator, and training step/training loop logic. For testing, I used the UrbanSounds8K dataset, specifically the street_music subset, which is typified by featuring short samples of live music recorded at different urban venues, and includes ambient noise like human chatter, laughter, cheering, singing, or passing traffic that somewhat complicate or muddy the audio. I then pre-processed and converted these audio files into "real" spectrograms using a script I'm including.

This model was to put it shortly, not a success. I'm including a report I wrote. It's an informal report, so formatting and technical writing are not publication worthy, but is complete with my references and some produced results. To summarize though, my model could not converge. Through a lot of experimentation and trial and error, as well just long training hours, I did produce some spectrogram-like images, where certain characteristics from my real spectrograms could be observed and noted in my generated "fake" spectrograms. However, they stillwere not perfect, and the generated audio was generally unpleasant to listen to. Not including the audio samples because they're not very helpful and don't expect anyone will want to be exposed to them.

Including some data-proprocessing files, like a scraper someone could adjust (like adjust the path files, directories, and keywords) to use for grabbing certain categories from the UrbanSounds8K dataset (and for the sake of completeness), and the actual core data-propocessing file that I used to clean up and convert the audio files into a dataset of spectrograms in a standardized format. Included a small sample size of these spectrograms. 

GANMODEL.py is, as the name suggests, the actual model I employed. It's a bit of a mess currently, and I expect I'll take an axe to it in the future as i make necessary changes and refine the model. Some results it created are included in my report and these are already the most promising/interesting results and can be lifted from there, so don't see the point in sharing generated specs in a folder of their own. The lowdown of the model though is that it's built using Tensorflow and Keras, and I did a fair amount of experimentation to get it to produce anything, including messing with the kernel size, stride length, learning rates, upsampling, number of layers, and batch size/sample size, but did face some practical limitations due to my GPU. 

Included in the metric folder are just the rmse.py and FID.py file that I used to, you guessed it, calcualte my RMSE and FID scores for my generated sepctrograms. Results are included in the report (note that again, it's not a super technical report, and just includes the averag score calculated, and not any exact scores for each and every one generated spectrogram.)

Uploading here on the off chance someone might find it online and use it as a potential resource (of what to do, but more likely what not to do. like I did during my own research for this project), to fill up my Github a bit, and to work on and update in the future.
